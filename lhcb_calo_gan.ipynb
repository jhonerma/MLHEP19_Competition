{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YoEHVVY35hAr"
   },
   "source": [
    "#### If you are using Google Colab, please, note following steps:\n",
    "0. Setup colab GPU in two clicks:\n",
    "\n",
    " 0.1 In `Edit` click on `Notebook settings`\n",
    " \n",
    " ![](https://github.com/SchattenGenie/mlhep2019_2_phase/blob/master/analysis/colab_gpu_1.png?raw=1)\n",
    "\n",
    " 0.2 Choose GPU in Hardware accelerator\n",
    " \n",
    " ![](https://github.com/SchattenGenie/mlhep2019_2_phase/blob/master/analysis/colab_gpu_2.png?raw=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 868
    },
    "colab_type": "code",
    "id": "ExOxmCSQ5hAy",
    "outputId": "3fb3e557-6693-49e0-ce87-c854b9aab46e"
   },
   "outputs": [],
   "source": [
    "#!wget https://raw.githubusercontent.com/SchattenGenie/mlhep2019_2_phase/master/analysis/calogan_metrics.py\n",
    "#!wget https://raw.githubusercontent.com/SchattenGenie/mlhep2019_2_phase/master/analysis/prd_score.py\n",
    "#!wget https://raw.githubusercontent.com/SchattenGenie/mlhep2019_2_phase/master/analysis/score.py\n",
    "#!wget https://github.com/SchattenGenie/mlhep2019_2_phase/raw/master/analysis/embedder.tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DU4sG52A5hA_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as utils\n",
    "from torchvision import transforms \n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm,tnrange,tqdm_notebook\n",
    "import time\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "sns.set()\n",
    "\n",
    "def one_hot(a, num_classes):\n",
    "    return np.squeeze(np.eye(num_classes)[a.reshape(-1)])\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gc5Etsq25hBG",
    "outputId": "2845a43f-45ef-46af-8f1c-e29ee1bce03f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "h2vwr61q5hBQ"
   },
   "source": [
    "#### A bit more steps to setup Google Colab\n",
    "\n",
    "1. Open this link: https://drive.google.com/open?id=13OVy1GlKFdOW_RjjOtg0AloHjOJMGSrj\n",
    "\n",
    "2. Add it to your Drive:\n",
    "![](http://www.digitalchaoscontrol.com/wp-content/uploads/2016/10/AddtoMyDrive.jpg)\n",
    "\n",
    "3. Uncomment and run following lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hidden": true,
    "id": "WykT-Zbn5hBS",
    "outputId": "d1a8abe1-30a2-418b-9b2a-1b2a1812bb6a"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpRU7n3A5hBa"
   },
   "source": [
    "## Data pathes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3irUMdR45hBc"
   },
   "outputs": [],
   "source": [
    "#train_data_path = '/gdrive/My Drive/mlhep2019_gan/data_train.npz'\n",
    "#val_data_path = '/gdrive/My Drive/mlhep2019_gan/data_val.npz'\n",
    "#test_data_path = '/gdrive/My Drive/mlhep2019_gan/data_test.npz'\n",
    "\n",
    "#data paths for folder on my local machine\n",
    "train_data_path = 'data/data_train.npz'\n",
    "val_data_path = 'data/data_val.npz'\n",
    "test_data_path = 'data/data_test.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ry9ffD3V5hBh"
   },
   "source": [
    "# Loading data\n",
    "\n",
    "Data is stored in `.npz`-format which is a special filetype for persisting multiple NumPy arrays on disk. \n",
    "\n",
    "More info: https://docs.scipy.org/doc/numpy/reference/generated/numpy.lib.format.html#module-numpy.lib.format.\n",
    "\n",
    "File `dat_train.npz` contains four arrays: \n",
    "\n",
    "  * `EnergyDeposit` - images of calorimeters responses\n",
    "  * `ParticleMomentum` - $p_x, p_y, p_z$ of initial partice\n",
    "  * `ParticlePoint` - $x, y$ of initial particle\n",
    "  * `ParticlePDG` - particle type(either $e^-$ or $\\gamma$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5C28P6iZ5hBk",
    "outputId": "01b8d0b4-d5a4-43de-9dc0-f6795a76c3d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EnergyDeposit', 'ParticlePoint', 'ParticleMomentum', 'ParticlePDG']\n"
     ]
    }
   ],
   "source": [
    "N = 50250\n",
    "\n",
    "data_train = np.load(train_data_path, allow_pickle=True)\n",
    "print(list(data_train.keys()))\n",
    "\n",
    "# [data_size, 900]\n",
    "EnergyDeposit = data_train['EnergyDeposit'][:N]\n",
    "# reshaping it as [data_size, channels, img_size_x, img_size_y]\n",
    "# channels are needed for pytorch conv2d-layers\n",
    "EnergyDeposit = EnergyDeposit.reshape(-1, 1, 30, 30)\n",
    "\n",
    "# [data_size, 3]\n",
    "ParticleMomentum = data_train['ParticleMomentum'][:N]\n",
    "\n",
    "# [data_size, 2]\n",
    "ParticlePoint = data_train['ParticlePoint'][:, :2][:N]\n",
    "\n",
    "# [data_size, 1]\n",
    "ParticlePDG = data_train['ParticlePDG'][:N]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mY7shoLm5hBt"
   },
   "source": [
    "## Load it to pytorch `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6x1XzAa5hBy"
   },
   "outputs": [],
   "source": [
    "EnergyDeposit = torch.tensor(EnergyDeposit).float()\n",
    "ParticleMomentum = torch.tensor(ParticleMomentum).float()\n",
    "ParticlePoint = torch.tensor(ParticlePoint).float()\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "calo_dataset = utils.TensorDataset(EnergyDeposit, ParticleMomentum, ParticlePoint)\n",
    "calo_dataloader = torch.utils.data.DataLoader(calo_dataset, batch_size=BATCH_SIZE, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0K3D4-xp5hB9"
   },
   "outputs": [],
   "source": [
    "for EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b in calo_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L9JtJxV75hCE"
   },
   "source": [
    "## Training GAN\n",
    "###### ...is not a simple matter\n",
    "\n",
    "It depends on architecture, loss, instance noise, augmentation and even luck(recommend to take a look https://arxiv.org/pdf/1801.04406.pdf)\n",
    "\n",
    "\n",
    "In this notebook I have prepared some basic parts that you could use for your experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qa6fI8vj5hCH"
   },
   "source": [
    "### Three types of losses for GANs\n",
    "\n",
    "https://medium.com/@jonathan_hui/gan-what-is-wrong-with-the-gan-cost-function-6f594162ce01\n",
    "\n",
    "There were proposed numerous loss functions to train GANs. In this notebook we have implemented three the most popular choices(but feel free to try other variants!):\n",
    "\n",
    "### `KL`:\n",
    "\n",
    "\n",
    "$$\\mathcal{L}_g = \\log(1 - \\mathrm{discriminator}(\\mathrm{gen}))$$\n",
    "\n",
    "$$\\mathcal{L}_d = - \\log(\\mathrm{discriminator}(\\mathrm{gen})) - \\log(1 - \\mathrm{discriminator}(\\mathrm{real}))$$\n",
    "\n",
    "\n",
    "### `REVERSED_KL`\n",
    "\n",
    "$$\\mathcal{L}_g = - \\log(\\mathrm{discriminator}(\\mathrm{gen}))$$\n",
    "\n",
    "$$\\mathcal{L}_d = - \\log(\\mathrm{discriminator}(\\mathrm{gen})) - \\log(1 - \\mathrm{discriminator}(\\mathrm{real}))$$\n",
    "\n",
    "\n",
    "### `WASSERSTEIN`\n",
    "\n",
    "$$\\mathcal{L}_g = - \\mathrm{discriminator}(\\mathrm{gen})$$\n",
    "\n",
    "$$\\mathcal{L}_d = \\mathrm{discriminator}(\\mathrm{gen}) - \\mathrm{discriminator}(\\mathrm{real})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1AMWG8p35hCL"
   },
   "outputs": [],
   "source": [
    "TASKS = ['KL', 'REVERSED_KL', 'WASSERSTEIN']\n",
    "\n",
    "TASK = 'WASSERSTEIN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qbeJlAZu5hCV"
   },
   "source": [
    "### Additional things for Wasserstein GAN\n",
    "\n",
    "To make `Wasserstein`-GAN works we suggest three options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6PA56mRs5hCW"
   },
   "outputs": [],
   "source": [
    "LIPSITZ_WEIGHTS = False\n",
    "clamp_lower, clamp_upper = -0.01, 0.01\n",
    "\n",
    "\n",
    "# https://arxiv.org/abs/1704.00028\n",
    "GRAD_PENALTY = True\n",
    "\n",
    "# https://arxiv.org/abs/1705.09367\n",
    "ZERO_CENTERED_GRAD_PENALTY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyXTGynB5hCc"
   },
   "source": [
    "#### Small hack that can speed-up training and improve generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZPOBudN15hCd"
   },
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1610.04490\n",
    "INSTANCE_NOISE = True\n",
    "\n",
    "def add_instance_noise(data, std=0.01):\n",
    "    return data + torch.distributions.Normal(0, std).sample(data.shape).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lEZJKn95hCl"
   },
   "outputs": [],
   "source": [
    "class GANLosses(object):\n",
    "    def __init__(self, task, device):\n",
    "        self.TASK = task\n",
    "        self.device = device\n",
    "    \n",
    "    def g_loss(self, discrim_output):\n",
    "        eps = 1e-10\n",
    "        if self.TASK == 'KL': \n",
    "            loss = torch.log(1 - discrim_output + eps).mean()    \n",
    "        elif self.TASK == 'REVERSED_KL':\n",
    "            loss = - torch.log(discrim_output + eps).mean()\n",
    "        elif self.TASK == 'WASSERSTEIN':\n",
    "            loss = - discrim_output.mean()\n",
    "        return loss\n",
    "\n",
    "    def d_loss(self, discrim_output_gen, discrim_output_real):\n",
    "        eps = 1e-10\n",
    "        if self.TASK in ['KL', 'REVERSED_KL']: \n",
    "            loss = - torch.log(discrim_output_real + eps).mean() - torch.log(1 - discrim_output_gen + eps).mean()\n",
    "        elif self.TASK == 'WASSERSTEIN':\n",
    "            loss = - (discrim_output_real.mean() - discrim_output_gen.mean())\n",
    "        return loss\n",
    "\n",
    "    def calc_gradient_penalty(self, discriminator, data_gen, inputs_batch, inp_data, lambda_reg = .1):\n",
    "        alpha = torch.rand(inp_data.shape[0], 1).to(self.device)\n",
    "        dims_to_add = len(inp_data.size()) - 2\n",
    "        for i in range(dims_to_add):\n",
    "            alpha = alpha.unsqueeze(-1)\n",
    "        # alpha = alpha.expand(inp_data.size())\n",
    "\n",
    "        interpolates = (alpha * inp_data + ((1 - alpha) * data_gen)).to(self.device)\n",
    "\n",
    "        interpolates.requires_grad = True\n",
    "\n",
    "        disc_interpolates = discriminator(interpolates, inputs_batch)\n",
    "\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).to(self.device),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_reg\n",
    "        return gradient_penalty\n",
    "    \n",
    "    def calc_zero_centered_GP(self, discriminator, data_gen, inputs_batch, inp_data, gamma_reg = .1):\n",
    "        \n",
    "        local_input = inp_data.clone().detach().requires_grad_(True)\n",
    "        disc_interpolates = discriminator(local_input, inputs_batch)\n",
    "        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=local_input,\n",
    "                                        grad_outputs=torch.ones(disc_interpolates.size()).to(self.device),\n",
    "                                        create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "        return gamma_reg / 2 * (gradients.norm(2, dim=1) ** 2).mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOb_GV9A5hCp"
   },
   "source": [
    "## Defining discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "397sHyip5hCs"
   },
   "outputs": [],
   "source": [
    "class ModelD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelD, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.lnorm1 = nn.LayerNorm([32,30,30])\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2)\n",
    "        self.lnorm2 = nn.LayerNorm([64,14,14])\n",
    "        self.conv3 = nn.Conv2d(64, 96, 3, padding=1)\n",
    "        self.lnorm3 = nn.LayerNorm([96,14,14])\n",
    "        self.conv4 = nn.Conv2d(96, 64, 3, padding=1)\n",
    "        self.lnorm4 = nn.LayerNorm([64,14,14])\n",
    "        self.conv5 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.lnorm5 = nn.LayerNorm([32,14,14])\n",
    "        self.conv6 = nn.Conv2d(32, 24, 3)\n",
    "        self.lnorm6 = nn.LayerNorm([24,12,12])\n",
    "\n",
    "                     \n",
    "        # size\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.fc1 = nn.Linear(3461, 3500) \n",
    "        self.fc2 = nn.Linear(3500, 3000)\n",
    "        self.fc3 = nn.Linear(3000, 3000)\n",
    "        self.fc4 = nn.Linear(3000, 1600)\n",
    "        self.fc5 = nn.Linear(1600, 800)\n",
    "        self.fc6 = nn.Linear(800, 400)\n",
    "        self.fc7 = nn.Linear(400, 400)\n",
    "        self.fc8 = nn.Linear(400,200)\n",
    "        self.fc9 = nn.Linear(200,100)\n",
    "        self.fc10 = nn.Linear(100, 50)\n",
    "        self.fc11 = nn.Linear(50,30)\n",
    "        self.fc12 = nn.Linear(30,1)\n",
    "        \n",
    "    def forward(self, EnergyDeposit, ParticleMomentum_ParticlePoint):\n",
    "        if TASK == 'WASSERSTEIN':\n",
    "            ### WGAN, no Batch Norm\n",
    "            EnergyDeposit = self.dropout(F.leaky_relu(self.lnorm1(self.conv1(EnergyDeposit))))\n",
    "            EnergyDeposit = self.dropout(F.leaky_relu(self.lnorm2(self.conv2(EnergyDeposit))))  \n",
    "            EnergyDeposit = self.dropout(F.leaky_relu(self.lnorm3(self.conv3(EnergyDeposit)))) \n",
    "            EnergyDeposit = self.dropout(F.leaky_relu(self.lnorm4(self.conv4(EnergyDeposit)))) \n",
    "            EnergyDeposit = self.dropout(F.leaky_relu(self.lnorm5(self.conv5(EnergyDeposit)))) \n",
    "        else:\n",
    "            ### Non WGAN, use Batch norm, needs to be updated before usage first\n",
    "            EnergyDeposit = self.dropout(F.leaky_relu(self.bn1(self.conv1(EnergyDeposit))))\n",
    "            EnergyDeposit = self.dropout(F.leaky_relu(self.bn2(self.conv2(EnergyDeposit))))\n",
    "        ### Rest of disc/critic\n",
    "        EnergyDeposit = F.leaky_relu(self.lnorm6(self.conv6(EnergyDeposit))) # 32, 9, 9\n",
    "        EnergyDeposit = EnergyDeposit.view(len(EnergyDeposit), -1)\n",
    "        \n",
    "        t = torch.cat([EnergyDeposit, ParticleMomentum_ParticlePoint], dim=1)\n",
    "        \n",
    "        t = self.dropout2(F.leaky_relu(self.fc1(t)))\n",
    "        t = self.dropout2(F.leaky_relu(self.fc2(t)))\n",
    "        t = self.dropout2(F.leaky_relu(self.fc3(t)))\n",
    "        t = self.dropout2(F.leaky_relu(self.fc4(t)))\n",
    "        t = self.dropout2(F.leaky_relu(self.fc5(t)))\n",
    "        t = self.dropout2(F.leaky_relu(self.fc6(t)))\n",
    "        t = self.dropout2(F.leaky_relu(self.fc7(t)))\n",
    "        t = self.dropout2(F.leaky_relu(self.fc8(t)))\n",
    "        t = self.dropout2(F.leaky_relu(self.fc9(t)))\n",
    "        t = self.dropout2(F.leaky_relu(self.fc10(t)))\n",
    "        t = self.dropout2(F.leaky_relu(self.fc11(t)))\n",
    "        if TASK == 'WASSERSTEIN':\n",
    "            return self.fc12(t)\n",
    "        else:\n",
    "            return torch.sigmoid(self.fc12(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deisc for my record\n",
    "# class ModelD(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(ModelD, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "#         self.bn1 = nn.BatchNorm2d(32)\n",
    "#         self.dropout = nn.Dropout(p=0.3)\n",
    "#         self.conv2 = nn.Conv2d(32, 64, 3, stride=2)\n",
    "#         self.bn2 = nn.BatchNorm2d(64)\n",
    "#         self.conv3 = nn.Conv2d(64, 64, 3)\n",
    "#         self.conv4 = nn.Conv2d(64, 32, 3)\n",
    "                \n",
    "#         # size\n",
    "#         self.fc1 = nn.Linear(2592 + 5, 1024) \n",
    "#         self.fc2 = nn.Linear(1024, 512)\n",
    "#         self.fc3 = nn.Linear(512, 256)\n",
    "#         self.fc4 = nn.Linear(256, 128)\n",
    "#         self.fc5 = nn.Linear(128, 64)\n",
    "#         self.fc6 = nn.Linear(64, 1)\n",
    "        \n",
    "#     def forward(self, EnergyDeposit, ParticleMomentum_ParticlePoint):\n",
    "#         if TASK == 'WASSERSTEIN':\n",
    "#             ### WGAN, no Batch Norm\n",
    "#             EnergyDeposit = self.dropout(F.leaky_relu((self.conv1(EnergyDeposit))))\n",
    "#             EnergyDeposit = self.dropout(F.leaky_relu(self.conv2(EnergyDeposit)))   \n",
    "#         else:\n",
    "#             ### Non WGAN, use Batch norm\n",
    "#             EnergyDeposit = self.dropout(F.leaky_relu(self.bn1(self.conv1(EnergyDeposit))))\n",
    "#             EnergyDeposit = self.dropout(F.leaky_relu(self.bn2(self.conv2(EnergyDeposit))))\n",
    "#         ### Rest of disc/critic\n",
    "#         EnergyDeposit = F.leaky_relu(self.conv3(EnergyDeposit))\n",
    "#         EnergyDeposit = F.leaky_relu(self.conv4(EnergyDeposit)) # 32, 9, 9\n",
    "#         EnergyDeposit = EnergyDeposit.view(len(EnergyDeposit), -1)\n",
    "        \n",
    "#         t = torch.cat([EnergyDeposit, ParticleMomentum_ParticlePoint], dim=1)\n",
    "        \n",
    "#         t = F.leaky_relu(self.fc1(t))\n",
    "#         t = F.leaky_relu(self.fc2(t))\n",
    "#         t = F.leaky_relu(self.fc3(t))\n",
    "#         t = F.leaky_relu(self.fc4(t))\n",
    "#         t = F.leaky_relu(self.fc5(t))\n",
    "#         if TASK == 'WASSERSTEIN':\n",
    "#             return self.fc6(t)\n",
    "#         else:\n",
    "#             return torch.sigmoid(self.fc6(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjfPUykA5hCx"
   },
   "source": [
    "## Defining generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-rIZ8ZO5hC0"
   },
   "outputs": [],
   "source": [
    "class ModelGConvTranspose(nn.Module):\n",
    "    def __init__(self, z_dim):\n",
    "        self.z_dim = z_dim\n",
    "        super(ModelGConvTranspose, self).__init__()\n",
    "        self.fc1 = nn.Linear(self.z_dim + 2 + 3, 512)\n",
    "        self.dense_norm1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 1024)\n",
    "        self.dense_norm2 = nn.BatchNorm1d(1024)\n",
    "        self.fc3 = nn.Linear(1024, 2048)\n",
    "        self.dense_norm3 = nn.BatchNorm1d(2048)\n",
    "        self.fc4 = nn.Linear(2048, 4096)\n",
    "        self.dense_norm4 = nn.BatchNorm1d(4096)\n",
    "        self.fc5 = nn.Linear(4096, 8192)\n",
    "        self.dense_norm5 = nn.BatchNorm1d(8192)\n",
    "        self.fc6 = nn.Linear(8192, 8192)\n",
    "        self.dense_norm6 = nn.BatchNorm1d(8192)\n",
    "        self.fc7 = nn.Linear(8192, 8192)\n",
    "        self.dense_norm7 = nn.BatchNorm1d(8192)\n",
    "        self.fc8 = nn.Linear(8192, 20736)\n",
    "        self.dense_norm8 = nn.BatchNorm1d(20736)\n",
    "        \n",
    "        #ConvTranspose can introduce artifacts in upscaling, old model, got high score 0.5366 on 13.08\n",
    "        #self.conv1 = nn.ConvTranspose2d(256, 256, 3, stride=2, output_padding=1)\n",
    "        #self.conv2 = nn.ConvTranspose2d(256, 128, 3)\n",
    "        #self.conv3 = nn.ConvTranspose2d(128, 64, 3)\n",
    "        #self.conv4 = nn.ConvTranspose2d(64, 32, 3)\n",
    "        #self.conv5 = nn.ConvTranspose2d(32, 16, 3)\n",
    "        #self.conv6 = nn.ConvTranspose2d(16, 1, 3)\n",
    "        \n",
    "        # Using upsample, conv2d to increase the image size, \n",
    "        self.refpad = nn.ReflectionPad2d(1)\n",
    "        self.conv1 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(256, 128, 1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 64, 1)\n",
    "        self.conv5 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(64, 32, 1)\n",
    "        self.conv7 = nn.Conv2d(32, 32 ,3, padding=1)\n",
    "        self.conv8 = nn.Conv2d(32, 16, 1)\n",
    "        self.conv9 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.conv10 = nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.conv11 = nn.Conv2d(16, 1, 3, padding=1)\n",
    "        self.upscale1 = nn.Upsample(size=(18,18), mode='nearest')\n",
    "        self.upscale2 = nn.Upsample(size=(30,30), mode='nearest')\n",
    "        \n",
    "        \n",
    "    def forward(self, z, ParticleMomentum_ParticlePoint):\n",
    "        x = F.leaky_relu(self.dense_norm1(self.fc1(\n",
    "            torch.cat([z, ParticleMomentum_ParticlePoint], dim=1)\n",
    "        )))\n",
    "        x = F.leaky_relu(self.dense_norm2(self.fc2(x)))\n",
    "        x = F.leaky_relu(self.dense_norm3(self.fc3(x)))\n",
    "        x = F.leaky_relu(self.dense_norm4(self.fc4(x)))\n",
    "        x = F.leaky_relu(self.dense_norm5(self.fc5(x)))\n",
    "        x = F.leaky_relu(self.dense_norm6(self.fc6(x)))\n",
    "        x = F.leaky_relu(self.dense_norm7(self.fc7(x)))\n",
    "        x = F.leaky_relu(self.dense_norm8(self.fc8(x)))\n",
    "        \n",
    "        EnergyDeposit = x.view(-1, 256, 9, 9)\n",
    "        \n",
    "        #EnergyDeposit = self.refpad(EnergyDeposit)\n",
    "        EnergyDeposit = F.leaky_relu(self.conv1(EnergyDeposit))\n",
    "        EnergyDeposit = self.upscale1(EnergyDeposit)\n",
    "        #EnergyDeposit = self.refpad(EnergyDeposit)\n",
    "        EnergyDeposit = F.leaky_relu(self.conv1(EnergyDeposit))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv2(EnergyDeposit))\n",
    "        EnergyDeposit = self.upscale2(EnergyDeposit)\n",
    "        #EnergyDeposit = self.refpad(EnergyDeposit)\n",
    "        EnergyDeposit = F.leaky_relu(self.conv3(EnergyDeposit))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv4(EnergyDeposit))\n",
    "        #EnergyDeposit = self.refpad(EnergyDeposit)\n",
    "        EnergyDeposit = F.leaky_relu(self.conv5(EnergyDeposit))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv6(EnergyDeposit))\n",
    "        #EnergyDeposit = self.refpad(EnergyDeposit)\n",
    "        EnergyDeposit = F.leaky_relu(self.conv7(EnergyDeposit))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv8(EnergyDeposit))\n",
    "        #EnergyDeposit = self.refpad(EnergyDeposit)\n",
    "        EnergyDeposit = F.leaky_relu(self.conv9(EnergyDeposit))\n",
    "        EnergyDeposit = F.leaky_relu(self.conv10(EnergyDeposit))\n",
    "        EnergyDeposit = self.conv11(EnergyDeposit)\n",
    "        \n",
    "        #old model, got high score 0.5366 on 13.08\n",
    "        #EnergyDeposit = F.leaky_relu(self.conv1(EnergyDeposit))\n",
    "        #EnergyDeposit = F.leaky_relu(self.conv2(EnergyDeposit))\n",
    "        #EnergyDeposit = F.leaky_relu(self.conv3(EnergyDeposit))\n",
    "        #EnergyDeposit = F.leaky_relu(self.conv4(EnergyDeposit))\n",
    "        #EnergyDeposit = F.leaky_relu(self.conv5(EnergyDeposit))\n",
    "        #EnergyDeposit = self.conv6(EnergyDeposit)\n",
    "\n",
    "        return EnergyDeposit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZ5O0FdI5hC4"
   },
   "source": [
    "## Check our models on one batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLJVneSp5hC5"
   },
   "outputs": [],
   "source": [
    "NOISE_DIM = 1000\n",
    "\n",
    "discriminator = ModelD().to(device)\n",
    "generator = ModelGConvTranspose(z_dim=NOISE_DIM).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b = EnergyDeposit_b.to(device), \\\n",
    "#                                                        ParticleMomentum_b.to(device), \\\n",
    "#                                                        ParticlePoint_b.to(device)\n",
    "#ParticleMomentum_ParticlePoint_b = torch.cat([ParticleMomentum_b.to(device), ParticlePoint_b.to(device)], dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L-d5xTDS5hDK",
    "outputId": "b346b815-8ce3-4e34-eb83-daddfce97b6d"
   },
   "outputs": [],
   "source": [
    "#EnergyDeposit_b.shape\n",
    "#ParticleMomentum_ParticlePoint_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b9rIPjoJ5hDR",
    "outputId": "771ed35c-5ca5-497c-c4af-66fcabecf6e8"
   },
   "outputs": [],
   "source": [
    "#discriminator(EnergyDeposit_b, ParticleMomentum_ParticlePoint_b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "l7ls-1275hDW",
    "outputId": "6ab76ae4-5ff3-44b3-f8ac-886de3087c88"
   },
   "outputs": [],
   "source": [
    "#noise = torch.randn(len(EnergyDeposit_b), NOISE_DIM).to(device)\n",
    "#generator(noise, ParticleMomentum_ParticlePoint_b).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JLFpghBK5hDf"
   },
   "source": [
    "## Defining optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qW2-Qfs5hDg"
   },
   "outputs": [],
   "source": [
    "learning_rate_dis = 1e-5\n",
    "learning_rate_gen = 1e-5\n",
    "\n",
    "#g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate_gen, weight_decay=1e-6)\n",
    "#d_optimizer = optim.SGD(discriminator.parameters(), lr=learning_rate_dis, weight_decay=1e-6)\n",
    "\n",
    "g_optimizer = optim.RMSprop(generator.parameters(), lr=learning_rate_gen, weight_decay=1e-5)\n",
    "d_optimizer = optim.RMSprop(discriminator.parameters(), lr=learning_rate_dis, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelGConvTranspose(\n",
       "  (fc1): Linear(in_features=1005, out_features=512, bias=True)\n",
       "  (dense_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=512, out_features=1024, bias=True)\n",
       "  (dense_norm2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc3): Linear(in_features=1024, out_features=2048, bias=True)\n",
       "  (dense_norm3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc4): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "  (dense_norm4): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc5): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "  (dense_norm5): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc6): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "  (dense_norm6): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc7): Linear(in_features=8192, out_features=8192, bias=True)\n",
       "  (dense_norm7): BatchNorm1d(8192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc8): Linear(in_features=8192, out_features=20736, bias=True)\n",
       "  (dense_norm8): BatchNorm1d(20736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (refpad): ReflectionPad2d((1, 1, 1, 1))\n",
       "  (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv6): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv7): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv8): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv9): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv11): Conv2d(16, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upscale1): Upsample(size=(18, 18), mode=nearest)\n",
       "  (upscale2): Upsample(size=(30, 30), mode=nearest)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(net):\n",
    "    with torch.no_grad(): \n",
    "        if type(net) == nn.Linear:\n",
    "            torch.nn.init.xavier_normal_(net.weight)\n",
    "            net.bias.data.fill_(0.001)\n",
    "        if type(net) == nn.Conv2d:\n",
    "            torch.nn.init.xavier_uniform_(net.weight)\n",
    "            net.bias.data.fill_(0.001)\n",
    "        if type(net) == nn.ConvTranspose2d:\n",
    "            torch.nn.init.xavier_uniform_(net.weight)\n",
    "            net.bias.data.fill_(0.001)\n",
    "        \n",
    "discriminator.apply(init_weights)\n",
    "generator.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Load Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights after training for image generation\n",
    "Load_Parameters = False\n",
    "\n",
    "# Resume training from a checkpoint\n",
    "Resume_Training = False\n",
    "# Otherwise start training from scratch\n",
    "\n",
    "# Save parameters after training\n",
    "Save_Parameters = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbtmyq005hDo"
   },
   "source": [
    "## Load scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1dbgDdy5hDr"
   },
   "outputs": [],
   "source": [
    "from prd_score import compute_prd, compute_prd_from_embedding, _prd_to_f_beta\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "DJ53UUBO5hD1"
   },
   "outputs": [],
   "source": [
    "dis_epoch_loss = []\n",
    "gen_epoch_loss = []\n",
    "predictions_dis = []\n",
    "predictions_gen = []\n",
    "prd_auc = [] \n",
    "\n",
    "def run_training(epochs, start=0):\n",
    "\n",
    "    # ===========================\n",
    "    # IMPORTANT PARAMETER:\n",
    "    # Number of D updates per G update\n",
    "    # ===========================\n",
    "    k_d, k_g = 10, 1\n",
    "\n",
    "    gan_losses = GANLosses(TASK, device)\n",
    "    transforming = transforms.RandomErasing(p=0.7,scale=(0.02,0.1))\n",
    "                \n",
    "    for epoch in tqdm(range(start, epochs)):\n",
    "        first = True\n",
    "        \n",
    "        for EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b in calo_dataloader:\n",
    "            with torch.no_grad():    \n",
    "                for i, pic in enumerate(EnergyDeposit_b):\n",
    "                    EnergyDeposit_b[i] = transforming(pic)\n",
    "            EnergyDeposit_b, ParticleMomentum_b, ParticlePoint_b = EnergyDeposit_b.to(device), \\\n",
    "                                                                   ParticleMomentum_b.to(device), \\\n",
    "                                                                   ParticlePoint_b.to(device)\n",
    "            ParticleMomentum_ParticlePoint_b = torch.cat([ParticleMomentum_b.to(device), ParticlePoint_b.to(device)], dim=1)\n",
    "            if first:\n",
    "                noise = torch.randn(len(EnergyDeposit_b), NOISE_DIM).to(device)\n",
    "                EnergyDeposit_gen = generator(noise, ParticleMomentum_ParticlePoint_b)\n",
    "                predictions_dis.append(\n",
    "                    list(discriminator(EnergyDeposit_b, ParticleMomentum_ParticlePoint_b).detach().cpu().numpy().ravel())\n",
    "                )\n",
    "\n",
    "                predictions_gen.append(\n",
    "                    list(discriminator(EnergyDeposit_gen, ParticleMomentum_ParticlePoint_b).detach().cpu().numpy().ravel())\n",
    "                )\n",
    "            # Optimize D\n",
    "            for _ in range(k_d):\n",
    "                noise = torch.randn(len(EnergyDeposit_b), NOISE_DIM).to(device)\n",
    "                EnergyDeposit_gen = generator(noise, ParticleMomentum_ParticlePoint_b)\n",
    "    \n",
    "                if INSTANCE_NOISE:\n",
    "                    EnergyDeposit_b = add_instance_noise(EnergyDeposit_b)\n",
    "                    EnergyDeposit_gen = add_instance_noise(EnergyDeposit_gen)\n",
    "                    \n",
    "                loss = gan_losses.d_loss(discriminator(EnergyDeposit_gen, ParticleMomentum_ParticlePoint_b),\n",
    "                                         discriminator(EnergyDeposit_b, ParticleMomentum_ParticlePoint_b))\n",
    "                if GRAD_PENALTY:\n",
    "                    grad_penalty = gan_losses.calc_gradient_penalty(discriminator,\n",
    "                                                                    EnergyDeposit_gen.data,\n",
    "                                                                    ParticleMomentum_ParticlePoint_b,\n",
    "                                                                    EnergyDeposit_b.data)\n",
    "                    loss += grad_penalty\n",
    "                    \n",
    "                elif ZERO_CENTERED_GRAD_PENALTY:\n",
    "                    grad_penalty = gan_losses.calc_zero_centered_GP(discriminator,\n",
    "                                                                    EnergyDeposit_gen.data,\n",
    "                                                                    ParticleMomentum_ParticlePoint_b,\n",
    "                                                                    EnergyDeposit_b.data)\n",
    "                    loss -= grad_penalty\n",
    "\n",
    "                d_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                d_optimizer.step()\n",
    "                if LIPSITZ_WEIGHTS:                    \n",
    "                    [p.data.clamp_(clamp_lower, clamp_upper) for p in discriminator.parameters()]\n",
    "\n",
    "            dis_epoch_loss.append(loss.item())\n",
    "\n",
    "            # Optimize G\n",
    "            for _ in range(k_g):\n",
    "                noise = torch.randn(len(EnergyDeposit_b), NOISE_DIM).to(device)\n",
    "                EnergyDeposit_gen = generator(noise, ParticleMomentum_ParticlePoint_b)\n",
    "                \n",
    "                if INSTANCE_NOISE:\n",
    "                    EnergyDeposit_b = add_instance_noise(EnergyDeposit_b)\n",
    "                    EnergyDeposit_gen = add_instance_noise(EnergyDeposit_gen)\n",
    "                \n",
    "                loss = gan_losses.g_loss(discriminator(EnergyDeposit_gen, ParticleMomentum_ParticlePoint_b))\n",
    "                g_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                g_optimizer.step()\n",
    "                \n",
    "            gen_epoch_loss.append(loss.item())\n",
    "            if first:\n",
    "                precision, recall = compute_prd_from_embedding(\n",
    "                    EnergyDeposit_gen.detach().cpu().numpy().reshape(BATCH_SIZE, -1), \n",
    "                    EnergyDeposit_b.detach().cpu().numpy().reshape(BATCH_SIZE, -1),\n",
    "                    num_clusters=30,\n",
    "                    num_runs=100)\n",
    "                prd_auc.append(auc(precision, recall))\n",
    "                first = False\n",
    "        \n",
    "        clear_output()\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.plot(dis_epoch_loss, label='dis_epoch_loss')\n",
    "        plt.plot(gen_epoch_loss, label='gen_epoch_loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.hist(predictions_dis[-1], bins=100, label='dis_prediction')\n",
    "        plt.hist(predictions_gen[-1], bins=100, label='gen_prediction')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        print(np.mean(predictions_dis[-1]), np.mean(predictions_gen[-1]))\n",
    "        \n",
    "        plt.figure(figsize=(12, 12))\n",
    "        plt.plot(prd_auc, label='prd_auc')\n",
    "        plt.plot()\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vmeoAvtd5hD5",
    "outputId": "dd61270a-0270-40ef-d5d7-0a5348af947b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "if Load_Parameters: #Loading weights after training for image generation\n",
    "    g_checkpoint = torch.load('./generator_params1908.pt')\n",
    "    generator.load_state_dict(g_checkpoint['model_state_dict'])\n",
    "    generator.eval()\n",
    "    d_checkpoint = torch.load('./discriminator_params1908.pt')\n",
    "    discriminator.load_state_dict(d_checkpoint['model_state_dict'])\n",
    "    discriminator.eval()\n",
    "                       \n",
    "                              \n",
    "\n",
    "elif Resume_Training: #Resume training from a checkpoint\n",
    "    g_checkpoint = torch.load('./generator_params.pt')\n",
    "    generator.load_state_dict(g_checkpoint['model_state_dict'])\n",
    "    g_optimizer.load_state_dict(g_checkpoint['optimizer_state_dict'])\n",
    "    startepoch = g_checkpoint['epoch']\n",
    "    gen_epoch_loss = g_checkpoint['loss']\n",
    "    prd_auc = g_checkpoint['prd_auc']\n",
    "    predictions_gen = g_checkpoint['prediction']\n",
    "    generator.eval()\n",
    "    generator.train()\n",
    "                                                                                         \n",
    "    d_checkpoint = torch.load('./discriminator_params.pt')\n",
    "    discriminator.load_state_dict(d_checkpoint['model_state_dict'])\n",
    "    d_optimizer.load_state_dict(d_checkpoint['optimizer_state_dict'])\n",
    "    dis_epoch_loss = d_checkpoint['loss']\n",
    "    predictions_dis = d_checkpoint['prediction']\n",
    "    discriminator.eval()\n",
    "    discriminator.train()\n",
    "      \n",
    "    run_training(4000, start=startepoch)\n",
    "              \n",
    "        \n",
    "        \n",
    "else: #Start training from scratch\n",
    "    run_training(4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Save_Parameters:\n",
    "    torch.save({'model_state_dict' : generator.state_dict(),\n",
    "                'optimizer_state_dict' : g_optimizer.state_dict(),\n",
    "                'epoch' : len(prd_auc)-1,\n",
    "                'loss' : gen_epoch_loss,\n",
    "                'prediction' : predictions_gen,\n",
    "                'prd_auc' : prd_auc\n",
    "               }, './generator_params.pt')\n",
    "    \n",
    "    torch.save({'model_state_dict' : discriminator.state_dict(),\n",
    "                'optimizer_state_dict' : d_optimizer.state_dict(),\n",
    "                'loss' : dis_epoch_loss,\n",
    "                'prediction' : predictions_dis\n",
    "                }, './discriminator_params.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vY6wzMdV5hD8"
   },
   "source": [
    "#### Transfer generator on CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m7wO3rWV5hD9"
   },
   "outputs": [],
   "source": [
    "generator_cpu = ModelGConvTranspose(z_dim=NOISE_DIM)\n",
    "generator_cpu.load_state_dict(generator.state_dict())\n",
    "generator_cpu.eval()\n",
    "generator_cpu.train(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdASA64Q5hEH"
   },
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wb8VIlq75hEJ"
   },
   "source": [
    "#### Validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sp_9T7B25hEJ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_val = np.load(val_data_path, allow_pickle=True)\n",
    "ParticleMomentum_val = torch.tensor(data_val['ParticleMomentum']).float()\n",
    "ParticlePoint_val = torch.tensor(data_val['ParticlePoint'][:, :2]).float()\n",
    "ParticleMomentum_ParticlePoint_val = torch.cat([ParticleMomentum_val, ParticlePoint_val], dim=1)\n",
    "calo_dataset_val = utils.TensorDataset(ParticleMomentum_ParticlePoint_val)\n",
    "calo_dataloader_val = torch.utils.data.DataLoader(calo_dataset_val, batch_size=1024, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    EnergyDeposit_val = []\n",
    "    for ParticleMomentum_ParticlePoint_val_batch in tqdm_notebook(calo_dataloader_val):\n",
    "        noise = torch.randn(len(ParticleMomentum_ParticlePoint_val_batch[0]), NOISE_DIM)\n",
    "        EnergyDeposit_val_batch = generator_cpu(noise, ParticleMomentum_ParticlePoint_val_batch[0]).detach().numpy()\n",
    "        EnergyDeposit_val.append(EnergyDeposit_val_batch)\n",
    "    np.savez_compressed('./data_val_prediction.npz', \n",
    "                        EnergyDeposit=np.concatenate(EnergyDeposit_val, axis=0))\n",
    "\n",
    "    del EnergyDeposit_val\n",
    "del data_val; del ParticleMomentum_val; del ParticlePoint_val; del ParticleMomentum_ParticlePoint_val;\n",
    "del calo_dataset_val; calo_dataloader_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46Z-6tL85hEM"
   },
   "source": [
    "#### Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0grM_SEL5hEO"
   },
   "outputs": [],
   "source": [
    "data_test = np.load(test_data_path, allow_pickle=True)\n",
    "ParticleMomentum_test = torch.tensor(data_test['ParticleMomentum']).float()\n",
    "ParticlePoint_test = torch.tensor(data_test['ParticlePoint'][:, :2]).float()\n",
    "ParticleMomentum_ParticlePoint_test = torch.cat([ParticleMomentum_test, ParticlePoint_test], dim=1)\n",
    "calo_dataset_test = utils.TensorDataset(ParticleMomentum_ParticlePoint_test)\n",
    "calo_dataloader_test = torch.utils.data.DataLoader(calo_dataset_test, batch_size=1024, shuffle=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    EnergyDeposit_test = []\n",
    "    for ParticleMomentum_ParticlePoint_test_batch in tqdm_notebook(calo_dataloader_test):\n",
    "        noise = torch.randn(len(ParticleMomentum_ParticlePoint_test_batch[0]), NOISE_DIM)\n",
    "        EnergyDeposit_test_batch = generator_cpu(noise, ParticleMomentum_ParticlePoint_test_batch[0]).detach().numpy()\n",
    "        EnergyDeposit_test.append(EnergyDeposit_test_batch)\n",
    "    np.savez_compressed('./data_test_prediction.npz', \n",
    "                        EnergyDeposit=np.concatenate(EnergyDeposit_test, axis=0))\n",
    "\n",
    "    del EnergyDeposit_test\n",
    "del data_test; del ParticleMomentum_test; del ParticlePoint_test; del ParticleMomentum_ParticlePoint_test;\n",
    "del calo_dataset_test; calo_dataloader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hH7chprT5hET"
   },
   "source": [
    "## `zip-zip` files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wTLFeUF95hEU"
   },
   "outputs": [],
   "source": [
    "!zip solution.zip data_val_prediction.npz data_test_prediction.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyVwAcC25hEX"
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('./solution.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNLcX2AN5hEa"
   },
   "source": [
    "# A few words about metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LS0U9c-b5hEc"
   },
   "source": [
    "### Lets generate some fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vP_-zOVQ5hEd"
   },
   "outputs": [],
   "source": [
    "EnergyDeposit_batch, ParticleMomentum_batch, ParticlePoint_batch =  next(iter(calo_dataloader))\n",
    "\n",
    "noise = torch.randn(len(ParticleMomentum_batch), NOISE_DIM)\n",
    "ParticleMomentum_ParticlePoint = torch.cat([ParticleMomentum_batch, \n",
    "                                            ParticlePoint_batch], dim=1)\n",
    "#EnergyDeposit_gen = generator_cpu(noise, ParticleMomentum_ParticlePoint)\n",
    "#with torch.no_grad():\n",
    "    #EnergyDeposit_gen = generator(noise.to('cuda'), ParticleMomentum_ParticlePoint.to('cuda'))\n",
    "EnergyDeposit_gen = generator_cpu(noise, ParticleMomentum_ParticlePoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nhxr6atW5hEg"
   },
   "outputs": [],
   "source": [
    "EnergyDeposit_gen = EnergyDeposit_gen.detach().cpu().numpy().reshape(-1,30, 30)\n",
    "EnergyDeposit = EnergyDeposit.detach().cpu().numpy().reshape(-1, 30, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnergyDeposit_gen.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aw8UIz775hEl"
   },
   "source": [
    "#### Plot one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images: np.ndarray, nrows: int=5, ncols: int=5,\n",
    "                shuffle: bool=True,\n",
    "                figure: matplotlib.figure.Figure=None) -> matplotlib.figure.Figure:\n",
    "#     \"\"\"\n",
    "#     Plots a subset of images.\n",
    "  \n",
    "#     Args:\n",
    "#         images[n_images, n_channels, width, height]: a dataset with images to plot\n",
    "#         nrows: number of images in a plotted row\n",
    "#         ncols: numer of images in a plotted colunm\n",
    "#         shuffle: if True draw a random subset of images, if False -- the first ones\n",
    "#         figure: if not None, it's used for plotting, if None, a new one is created\n",
    "        \n",
    "#     Returns:\n",
    "#         a figure containing the plotted images\n",
    "#     \"\"\"\n",
    "    if shuffle:\n",
    "        images_to_plot = images[np.random.permutation(len(images))[:nrows*ncols]]\n",
    "    else:\n",
    "        images_to_plot = images[:nrows * ncols]\n",
    "    h, w = images_to_plot.shape[1:]\n",
    "    if figure is None:\n",
    "        figure = plt.figure(figsize=(16,16))\n",
    "    axes = figure.subplots(nrows=nrows, ncols=ncols)\n",
    "    figure.subplots_adjust(left=0.02, bottom=0.06, right=0.98, top=0.94, wspace=0.3)\n",
    "    for row_idx, ax_row in enumerate(axes):\n",
    "        for col_idx, ax in enumerate(ax_row):\n",
    "            im = ax.imshow(images_to_plot[row_idx + ncols*col_idx].transpose(0, 1),\n",
    "                      interpolation=\"none\")\n",
    "            figure.colorbar(im , ax=ax)\n",
    "    #return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PD4wK8mz5hEm"
   },
   "outputs": [],
   "source": [
    "plot_images(EnergyDeposit_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(EnergyDeposit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8KcBQQZ5hEo"
   },
   "source": [
    "## Calculate PRD score between these batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1e0pI9dD5hEp"
   },
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.batchnorm0 = nn.BatchNorm2d(1)\n",
    "        self.conv1 = nn.Conv2d(1, 16, 2, stride=2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 2, stride=2)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 2, stride=2)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 256) \n",
    "        self.batchnorm4 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 2 + 3)\n",
    "        self.fc5 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.batchnorm0(self.dropout(x))\n",
    "        x = self.batchnorm1(self.dropout(F.relu(self.conv1(x))))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x)) # 64, 5, 5\n",
    "        x = x.view(len(x), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm4(self.dropout(F.relu(self.fc1(x))))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return self.fc4(x), self.fc5(x)\n",
    "    \n",
    "    def get_encoding(self, x):\n",
    "        x = self.batchnorm0(self.dropout(x))\n",
    "        x = self.batchnorm1(self.dropout(F.relu(self.conv1(x))))\n",
    "        x = self.batchnorm2(F.relu(self.conv2(x)))\n",
    "        x = self.batchnorm3(F.relu(self.conv3(x)))\n",
    "        x = F.relu(self.conv4(x)) # 64, 5, 5\n",
    "        x = x.view(len(x), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm4(self.dropout(F.relu(self.fc1(x))))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "def load_embedder(path):\n",
    "    embedder = torch.load(path)\n",
    "    embedder.eval()\n",
    "    return embedder\n",
    "\n",
    "embedder = load_embedder('./embedder.tp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SWMcZda85hEs"
   },
   "outputs": [],
   "source": [
    "data_real = embedder.get_encoding(torch.tensor(EnergyDeposit_batch).float().view(-1, 1, 30, 30)).detach().numpy()\n",
    "data_fake = embedder.get_encoding(torch.tensor(EnergyDeposit_gen).float().view(-1, 1, 30, 30)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ob7m7LcO5hEv"
   },
   "outputs": [],
   "source": [
    "def plot_pr_aucs(precisions, recalls):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    pr_aucs = []\n",
    "    for i in range(len(recalls)):\n",
    "        plt.step(recalls[i], precisions[i], color='b', alpha=0.2,  label='PR-AUC={}'.format(auc(precisions[i], recalls[i])))\n",
    "        pr_aucs.append(auc(precisions[i], recalls[i]))\n",
    "    plt.step(np.mean(recalls, axis=0), np.mean(precisions, axis=0), color='r', alpha=1,  label='average')\n",
    "    plt.fill_between(np.mean(recalls, axis=0), \n",
    "                     np.mean(precisions, axis=0) - np.std(precisions, axis=0) * 3,\n",
    "                     np.mean(precisions, axis=0) + np.std(precisions, axis=0) * 3, color='g', alpha=0.2,  label='std')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "\n",
    "    # plt.ylim([0.0, 1.05])\n",
    "    # plt.xlim([0.0, 1.0])\n",
    "    print(np.mean(pr_aucs), np.std(pr_aucs))\n",
    "    plt.legend()\n",
    "    \n",
    "    return pr_aucs\n",
    "\n",
    "def calc_pr_rec(data_real, data_fake, num_clusters=20, num_runs=10, NUM_RUNS=10):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    for i in tqdm_notebook(range(NUM_RUNS)):\n",
    "        precision, recall = compute_prd_from_embedding(data_real, data_fake, num_clusters=num_clusters, num_runs=num_runs)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAhy0rOz5hE0"
   },
   "outputs": [],
   "source": [
    "precisions, recalls = calc_pr_rec(data_real, data_fake, num_clusters=100, num_runs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMcpt32M5hE2"
   },
   "outputs": [],
   "source": [
    "pr_aucs = plot_pr_aucs(precisions, recalls)\n",
    "plt.title('Num_clusters={}, num_runs={}, first third'.format(100, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jqwbvem75hE7"
   },
   "outputs": [],
   "source": [
    "pr_aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgxylZMg5hFE"
   },
   "source": [
    "## Physical metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqy_D6qB5hFF"
   },
   "outputs": [],
   "source": [
    "import matplotlib.lines as mlines\n",
    "def newline(p1, p2):\n",
    "    ax = plt.gca()\n",
    "    xmin, xmax = ax.get_xbound()\n",
    "\n",
    "    if(p2[0] == p1[0]):\n",
    "        xmin = xmax = p1[0]\n",
    "        ymin, ymax = ax.get_ybound()\n",
    "    else:\n",
    "        ymax = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmax-p1[0])\n",
    "        ymin = p1[1]+(p2[1]-p1[1])/(p2[0]-p1[0])*(xmin-p1[0])\n",
    "\n",
    "    l = mlines.Line2D([xmin,xmax], [ymin,ymax])\n",
    "    ax.add_line(l)\n",
    "    return l\n",
    "\n",
    "def plot_axes_for_shower(ecal, point, p):\n",
    "    x = np.linspace(-14.5, 14.5, 30)\n",
    "    y = np.linspace(-14.5, 14.5, 30)\n",
    "\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    zoff = 25.\n",
    "    ipic = 3\n",
    "    orth = np.array([-p[1], p[0]])\n",
    "\n",
    "    pref = point[:2] + p[:2] * zoff / p[2]\n",
    "\n",
    "    p1 = pref - 10 * p[:2]\n",
    "    p2 = pref + 10 * p[:2]\n",
    "    p3 = pref - 10 * orth\n",
    "    p4 = pref + 10 * orth\n",
    "\n",
    "    plt.contourf(xx, yy, np.log(ecal + 1), cmap=plt.cm.inferno)\n",
    "    newline(p1, p2)\n",
    "    newline(p3, p4)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "841Z9xvY5hFH"
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "plot_axes_for_shower(EnergyDeposit[idx], point=ParticlePoint[idx].detach().numpy(),\n",
    "                     p=ParticleMomentum[idx].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hI5kMTwW5hFJ"
   },
   "outputs": [],
   "source": [
    "from calogan_metrics import get_assymetry, get_shower_width, get_sparsity_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lTBxtwz25hFU"
   },
   "outputs": [],
   "source": [
    "assym = get_assymetry(EnergyDeposit, ParticleMomentum.detach().numpy(), ParticlePoint.detach().numpy(), orthog=False)\n",
    "assym_ortho = get_assymetry(EnergyDeposit, ParticleMomentum.detach().numpy(), ParticlePoint.detach().numpy(), orthog=True)\n",
    "sh_width = get_shower_width(EnergyDeposit, ParticleMomentum.detach().numpy(), ParticlePoint.detach().numpy(), orthog=False)\n",
    "sh_width_ortho = get_shower_width(EnergyDeposit, ParticleMomentum.detach().numpy(), ParticlePoint.detach().numpy(), orthog=True)\n",
    "sparsity_level = get_sparsity_level(EnergyDeposit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7HmbU3q5hFY"
   },
   "source": [
    "## Longitudual cluster asymmetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8k41qX3T5hFY"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "plt.hist(assym, bins=50, range=[-1, 1], color='red', alpha=0.3, density=True, label='MC');\n",
    "plt.xlabel('Longitudual cluster asymmetry')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ultoKH-h5hFa"
   },
   "source": [
    "## Transverse cluster asymmetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rf_B7byC5hFb"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "plt.hist(assym_ortho, bins=50, range=[-1, 1], color='red', alpha=0.3, density=True, label='MC');\n",
    "plt.xlabel('Transverse cluster asymmetry')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ujMrqtPw5hFc"
   },
   "source": [
    "## Cluster longitudual width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0c2Auvd5hFd"
   },
   "outputs": [],
   "source": [
    "plt.hist(sh_width, bins=50, range=[0, 15], density=True, alpha=0.3, color='red', label='MC');\n",
    "plt.title('Shower longitudial width')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Cluster longitudual width [cm]')\n",
    "plt.ylabel('Arbitrary units')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvjR-7t45hFf"
   },
   "source": [
    "## Cluster trasverse width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vQVh3vI5hFg"
   },
   "outputs": [],
   "source": [
    "plt.hist(sh_width_ortho, bins=50, range=[0,10], density=True, alpha=0.3, color='blue', label='MC');\n",
    "#plt.title('Shower transverse width')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Cluster trasverse width [cm]')\n",
    "plt.ylabel('Arbitrary units')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yf1azmo35hFh"
   },
   "source": [
    "## Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yoQXTjQE5hFj"
   },
   "outputs": [],
   "source": [
    "alphas = np.log(np.logspace(-5, -1, 20))\n",
    "means_r = np.mean(sparsity_level, axis=1)\n",
    "stddev_r = np.std(sparsity_level, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9-1Uk9f5hFm"
   },
   "outputs": [],
   "source": [
    "plt.plot(alphas, means_r, color='red')\n",
    "plt.fill_between(alphas, means_r-stddev_r, means_r+stddev_r, color='red', alpha=0.3)\n",
    "plt.legend(['MC'])\n",
    "plt.title('Sparsity')\n",
    "plt.xlabel('log10(Threshold/GeV)')\n",
    "plt.ylabel('Fraction of cells above threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p3Il3IAD5hFo"
   },
   "outputs": [],
   "source": [
    "from calogan_metrics import get_physical_stats\n",
    "real_phys_stats = get_physical_stats(EnergyDeposit_batch, ParticleMomentum_batch.detach().numpy(), ParticlePoint_batch.detach().numpy())\n",
    "gen_phys_stats = get_physical_stats(EnergyDeposit_gen, ParticleMomentum_batch.detach().numpy(), ParticlePoint_batch.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eyPpvkpm5hFv"
   },
   "outputs": [],
   "source": [
    "precisions, recalls = calc_pr_rec(real_phys_stats, gen_phys_stats, num_clusters=100, num_runs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fn2Paxg55hFw"
   },
   "outputs": [],
   "source": [
    "pr_aucs = plot_pr_aucs(precisions, recalls)\n",
    "plt.title('Num_clusters={}, num_runs={}, first third'.format(100, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NlDiE6cg5hF0"
   },
   "outputs": [],
   "source": [
    "pr_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Copy of lhcb_calo_gan.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
